{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b889eb53",
   "metadata": {},
   "source": [
    "<center><h2>Sorbonne université</h2></center>\n",
    "<center><h4>IAR - Intelligence artificielle pour la robotique</h4></center>\n",
    "<center><h4>M2 Artificial Intelligence</h4></center>\n",
    "<center><h1>Apprentissage par renforcement profond</h1></center>\n",
    "<center><h3>LunarLander-v2</h3></center>\n",
    "<br />\n",
    "<center><h4>Thomas CORCORAL - <a href=\"https://www.linkedin.com/in/thomas-corcoral/?locale=en_US\">linkedIn</a></h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3accd4",
   "metadata": {},
   "source": [
    "# 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a749827",
   "metadata": {},
   "source": [
    "## 1.1 git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c30170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/ThomasCorcoral/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mv rl-baselines3-zoo/* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r rl-baselines3-zoo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir data/policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266955b3",
   "metadata": {},
   "source": [
    "## 1.2 pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f02e24",
   "metadata": {},
   "source": [
    "Some corrections to support different environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7602b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install Box2D\n",
    "!pip3 install box2d-py\n",
    "!pip3 install gym[all]\n",
    "!pip3 install gym[Box_2D] # To support all envs (some problems with Box_2D on Kaggle)\n",
    "!pip install sb3-contrib\n",
    "!pip install pyglet\n",
    "!pip install huggingface_hub\n",
    "!pip install huggingface_sb3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2977e0",
   "metadata": {},
   "source": [
    "## 1.3 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e140f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c1201",
   "metadata": {},
   "source": [
    "# 2. Premiers essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --algo DQN --env LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp logs/dqn/LunarLander-v2_1/best_model.zip ./data/policies/LunarLander-v2#dqn#dqn1.zip\n",
    "!python sb3_evaluator.py\n",
    "!rm ./data/policies/LunarLander-v2#dqn#dqn1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --algo PPO --env LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp logs/ppo/LunarLander-v2_1/best_model.zip ./data/policies/LunarLander-v2#ppo#ppo1.zip\n",
    "!python sb3_evaluator.py\n",
    "!rm ./data/policies/LunarLander-v2#ppo#ppo1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --algo PPO --env LunarLanderContinuous-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp logs/ppo/LunarLanderContinuous-v2_1/best_model.zip ./data/policies/LunarLanderContinuous-v2#ppo#ppo1.zip\n",
    "!python sb3_evaluator.py\n",
    "!rm ./data/policies/LunarLanderContinuous-v2#ppo#ppo1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --algo SAC --env LunarLanderContinuous-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp logs/ppo/LunarLanderContinuous-v2_1/best_model.zip ./data/policies/LunarLanderContinuous-v2#sac#sac1.zip\n",
    "!python sb3_evaluator.py\n",
    "!rm ./data/policies/LunarLanderContinuous-v2#sac#sac1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3be563",
   "metadata": {},
   "source": [
    "# 3. Optimisation des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cd096",
   "metadata": {},
   "source": [
    "## 3.1 Optimisation avec les scripts sb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --algo ppo --env LunarLander-v2 -n 100000 -optimize --n-trials 10000 --n-jobs 2 --sampler tpe --pruner median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --algo sac --env LunarLanderContinuous-v2 -n 100000 -optimize --n-trials 10000 --n-jobs 2 --sampler tpe --pruner median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161d910",
   "metadata": {},
   "source": [
    "## 3.2 Optimisation LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = 100000\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "ENV_ID = \"LunarLander-v2\"\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for PPO hyperparameters.\"\"\"\n",
    "    n_steps = 2 ** trial.suggest_int(\"n_steps\", 3, 11)\n",
    "    batch_size = 2 ** trial.suggest_int(\"batch_size\", 5, 9)\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.8, 0.83, 0.85, 0.87, 0.9, 0.93, 0.95, 0.98])\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999, log=True)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 4, 20)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0, 0.06)\n",
    "    learning_rate =  trial.suggest_float(\"lr\", 1e-7, 0.001, log=True)\n",
    "    clip_range = trial.suggest_float(\"clip_range\", 0, 1)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5, log=True)\n",
    "    vf_coef = trial.suggest_float(\"vf_coef\", 0, 1)\n",
    "    \n",
    "    log_std_init = trial.suggest_float(\"log_std_init\", -4, -1)\n",
    "    ortho_init = False\n",
    "    activation_fn = nn.ReLU\n",
    "    which_net_arch = trial.suggest_categorical(\"net_arch\", [\"big\", \"small\"])\n",
    "    \n",
    "    if which_net_arch == \"big\":\n",
    "        net_arch=[dict(pi=[256, 256], vf=[256, 256])]\n",
    "    else:\n",
    "        net_arch=[dict(pi=[64, 64], vf=[64, 64])]\n",
    "    activation_fn = nn.ReLU\n",
    "    ortho_init = False\n",
    "    \n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"gae_lambda_\", gae_lambda)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gae_lambda\": gae_lambda, \n",
    "        \"gamma\": gamma,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "            \"ortho_init\": ortho_init,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"Callback used for evaluating and reporting a trial.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    kwargs.update(sample_ppo_params(trial))\n",
    "    model = PPO(**kwargs)\n",
    "    eval_env = gym.make(ENV_ID)\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73554cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba1c64a",
   "metadata": {},
   "source": [
    "## 3.3 Optimisation LunarLanderContinuous-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ccd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = 50000\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "ENV_ID = \"LunarLanderContinuous-v2\"\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sac_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for SAC hyperparameters.\"\"\"\n",
    "    # policy, env \n",
    "    \n",
    "    # (Union[float, Callable[[float], float]]) – learning rate for adam optimizer,  \n",
    "    # the same learning rate will be used for all networks (Q-Values, Actor and Value \n",
    "    # function) it can be a function of the current progress remaining (from 1 to 0)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    # INT - size of the replay buffer\n",
    "    buffer_size = 2 ** trial.suggest_int(\"buffer_size\", 17, 21)\n",
    "    # INT - how many steps of the model to collect transitions for before learning starts\n",
    "    learning_starts = 100 * trial.suggest_int(\"learning_starts\", 1, 50)\n",
    "    # INT - Minibatch size for each gradient update\n",
    "    batch_size = 256\n",
    "    # FLOAT - the soft update coefficient (“Polyak update”, between 0 and 1)\n",
    "    tau = trial.suggest_float(\"tau\", 0.0001, 0.2, log=True)\n",
    "    # FLOAT - the discount factor\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999, log=True)\n",
    "    # INT - Update the model every train_freq steps\n",
    "    train_freq = trial.suggest_int(\"train_freq\", 1, 10)\n",
    "    # INT - How many gradient steps to do after each rollout\n",
    "    gradient_steps = trial.suggest_int(\"gradient_steps\", 1, 10)\n",
    "    # FLOAT - Entropy regularization coefficient\n",
    "    # ent_coef = trial.suggest_float(\"ent_coef\", 0.05, 0.15, log=True)\n",
    "    # INT - update the target network every target_network_update_freq gradient steps.\n",
    "    # target_update_interval = trial.suggest_int(\"target_update_interval\", 1, 100)\n",
    "    # FLOAT - target entropy when learning ent_coef\n",
    "    # target_entropy = trial.suggest_float(\"target_entropy\", 0.05, 0.15, log=True)\n",
    "    # BOOL - Whether to use generalized State Dependent Exploration (gSDE) instead of action noise exploration\n",
    "    use_sde = True\n",
    "    # INT - Sample a new noise matrix every n steps when using gSDE\n",
    "    sde_sample_freq = trial.suggest_int(\"sde_sample_freq\", 1, 10)\n",
    "    # BOOL - Whether to use gSDE instead of uniform sampling during the warm up phase (before learning starts)\n",
    "    use_sde_at_warmup = trial.suggest_categorical(\"use_sde_at_warmup\", [True, False])\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\" : learning_rate, \n",
    "        \"buffer_size\" : buffer_size, \n",
    "        \"learning_starts\" : learning_starts,\n",
    "        \"batch_size\" : batch_size,\n",
    "        \"tau\" : tau,\n",
    "        \"gamma\" : gamma,\n",
    "        \"train_freq\" : train_freq,\n",
    "        \"gradient_steps\" : gradient_steps,\n",
    "        \"use_sde\" : use_sde,\n",
    "        \"sde_sample_freq\" : sde_sample_freq,\n",
    "        \"use_sde_at_warmup\" : use_sde_at_warmup,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"Callback used for evaluating and reporting a trial.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    kwargs.update(sample_sac_params(trial))\n",
    "    model = SAC(**kwargs)\n",
    "    eval_env = gym.make(ENV_ID)\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c191a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfb511",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed16393",
   "metadata": {},
   "source": [
    "# 4. Modification dynamique des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f2952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cb8ad7b",
   "metadata": {},
   "source": [
    "## 4.1 Learning rate pour PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5be366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value):\n",
    "    if isinstance(initial_value, str):\n",
    "        initial_value = float(initial_value)\n",
    "\n",
    "    def func(progress):\n",
    "        return progress * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "def lrsched():\n",
    "    def reallr(progress):\n",
    "        lr = 0.0004\n",
    "        if progress < 0.8:\n",
    "            lr = 0.0003\n",
    "        if progress < 0.6:\n",
    "            lr = 0.0002\n",
    "        if progress < 0.4:\n",
    "            lr = 0.0001\n",
    "        if progress < 0.2:\n",
    "            lr = 0.00005\n",
    "        return lr\n",
    "    return reallr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "n_envs = 32\n",
    "env = make_vec_env(env_id, n_envs=n_envs)\n",
    "eval_envs = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "eval_freq = int(1e5)\n",
    "eval_freq = max(eval_freq // n_envs, 1)\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_envs,\n",
    "    best_model_save_path=\"./\",\n",
    "    eval_freq=eval_freq,\n",
    "    n_eval_episodes=10,\n",
    ")\n",
    "\n",
    "model_ppo = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    n_epochs=8,\n",
    "    learning_rate=lrsched(),\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[dict(pi=[64, 64, 64, 64], vf=[64])]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1852c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_ppo.learn(total_timesteps=5000000, callback=eval_callback)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv best_model.zip ./data/policies/LunarLander-v2#ppo#Corcoral_Kostadinovic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc53847",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sb3_evaluator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa882cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./data/policies/LunarLander-v2#ppo#Corcoral_Kostadinovic.zip ./rl-trained-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12d958",
   "metadata": {},
   "source": [
    "## 4.2 Learning rate pour SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff965d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value):\n",
    "\n",
    "    if isinstance(initial_value, str):\n",
    "        initial_value = float(initial_value)\n",
    "\n",
    "    def func(progress):\n",
    "        return progress * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "def lrsched():\n",
    "    def reallr(progress):\n",
    "        lr = 0.005\n",
    "        if progress < 0.8:\n",
    "            lr = 0.003\n",
    "        if progress < 0.6:\n",
    "            lr = 0.001\n",
    "        if progress < 0.4:\n",
    "            lr = 0.0005\n",
    "        if progress < 0.2:\n",
    "            lr = 0.0001\n",
    "        return lr\n",
    "    return reallr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env_id = \"LunarLanderContinuous-v2\"\n",
    "n_envs = 32\n",
    "env = make_vec_env(env_id, n_envs=n_envs)\n",
    "eval_envs = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "eval_freq = int(1e5)\n",
    "eval_freq = max(eval_freq // n_envs, 1)\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_envs,\n",
    "    best_model_save_path=\"./\",\n",
    "    eval_freq=eval_freq,\n",
    "    n_eval_episodes=10,\n",
    ")\n",
    "\n",
    "model_sac = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    train_freq=9,\n",
    "    gradient_steps=8,\n",
    "    sde_sample_freq=5,\n",
    "    use_sde_at_warmup=False,\n",
    "    use_sde=False,\n",
    "    batch_size=256,\n",
    "    buffer_size=262144,\n",
    "    ent_coef='auto',\n",
    "    gamma=0.9877986493994404,\n",
    "    tau=0.0039251709137456195,\n",
    "    learning_rate=lrsched(),\n",
    "    learning_starts=2800,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[400, 300]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e11cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_sac.learn(total_timesteps=5000000, callback=eval_callback)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv best_model.zip ./data/policies/LunarLanderContinuous-v2#ppo#Corcoral_Kostadinovic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sb3_evaluator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./data/policies/LunarLander-v2#ppo#Corcoral_Kostadinovic.zip ./rl-trained-agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
